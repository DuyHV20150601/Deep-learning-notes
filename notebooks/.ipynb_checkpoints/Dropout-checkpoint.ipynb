{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Trong quá trình training một model luôn luôn phải có 2 tập data , train data(tập này sử  dụng mục đích để 'dạy' cho model học)và test data(tập này cho ta thấy được việc dự đoán chính xác của model trên tập mà model chưa được dạy).Sau khi quá trình training hoàn tất model sẽ gặp phải 2 tình huống không mong muốn xảy ra là overfitting hoặc underfitting.\n",
    "    Việc xuật hiện 2 tình huống overfitting và underfitting cũng thường xảy ra trong cuộc sống hằng ngày.Ví dụ :\n",
    "    -Có 2 bạn học sinh là A và B tham gia kì thi tốt nghiệp THPT 2019 vừa qua.Bạn A cố gằng cày cố các đề thi năm trước lẫn các bài tập và làm đi làm lại các đề ấy nhiều lần đến khi bạn ấy chỉ nhìn sơ qua cũng biết kết quả không cần tính toán nhưng đến kì thi thật các bài chưa gặp bạn ấy lại không làm tốt nhưng những bài bạn ấy đã gặp thì làm tốt, trường hợp này thì đa phần sĩ tử khi đi thi đều gặp không phải hiếm.Bạn B cũng cày cố ôn bài các bài tập của thầy cô cho và các đề năm trước .Khi đến kì thi thật thì các bài mà bạn đó chưa gặp lại có xu hướng làm tốt hơn con những bài mặc dù đã gặp qua thì làm không tốt bằng, trường hợp này cũng không ít khi đi thi nên có câu 'học tài thi phận'.\n",
    "    Ở ví dụ trên là minh họa cho 2 model , model A là bị overfitting tập train đưa ra kết quả quá tốt nhưng tập test thì kết quả không tốt bằng còn model B là bị underfitting tập train kết quả không tốt bằng tập test.\n",
    "    Để giải quyết 2 trường hợp trên có một vài phương pháp giúp model được tốt hơn :\n",
    "    - Với Underfitting:\n",
    "        -Giảm lượng tham số của model xuống , ví dụ nếu dùng neural network chỉ cần giảm lượng layer hoặc số node trong một layer .\n",
    "        -Tăng số lượng epochs (giống như vòng lập của quá trình trainning) trong quá trình training lên để model được học lâu hơn.\n",
    "    - Với overfitting :\n",
    "        - Stopping early , phương pháp này đơn giản nhất chỉ cần lưu lại kết quả của những epoch trước đó và giữ trọng số tốt nhất.\n",
    "        - Regual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x > 0)*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_deri(x):\n",
    "    return x>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train =  X_train[0:1000].reshape(1000,28*28)/255\n",
    "images_test = X_test[0:10000].reshape(10000,28*28)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = tf.keras.utils.to_categorical(y_train[0:1000],10)\n",
    "labels_test = tf.keras.utils.to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/Train-loss : 0.717,accuracy : 0.569.Test-Loss : 0.628,accuracy : 0.668 \n",
      "10/Train-loss : 0.331,accuracy : 0.895.Test-Loss : 0.449,accuracy : 0.815 \n",
      "20/Train-loss : 0.271,accuracy : 0.928.Test-Loss : 0.42,accuracy : 0.826 \n",
      "30/Train-loss : 0.239,accuracy : 0.944.Test-Loss : 0.413,accuracy : 0.833 \n",
      "40/Train-loss : 0.218,accuracy : 0.956.Test-Loss : 0.413,accuracy : 0.832 \n",
      "50/Train-loss : 0.202,accuracy : 0.964.Test-Loss : 0.417,accuracy : 0.83 \n",
      "60/Train-loss : 0.191,accuracy : 0.972.Test-Loss : 0.422,accuracy : 0.826 \n",
      "70/Train-loss : 0.182,accuracy : 0.978.Test-Loss : 0.428,accuracy : 0.821 \n",
      "80/Train-loss : 0.175,accuracy : 0.982.Test-Loss : 0.434,accuracy : 0.819 \n",
      "90/Train-loss : 0.169,accuracy : 0.984.Test-Loss : 0.441,accuracy : 0.815 \n",
      "100/Train-loss : 0.164,accuracy : 0.988.Test-Loss : 0.447,accuracy : 0.812 \n",
      "110/Train-loss : 0.16,accuracy : 0.989.Test-Loss : 0.454,accuracy : 0.809 \n",
      "120/Train-loss : 0.157,accuracy : 0.989.Test-Loss : 0.461,accuracy : 0.806 \n",
      "130/Train-loss : 0.154,accuracy : 0.99.Test-Loss : 0.467,accuracy : 0.802 \n",
      "140/Train-loss : 0.151,accuracy : 0.991.Test-Loss : 0.473,accuracy : 0.799 \n",
      "150/Train-loss : 0.149,accuracy : 0.992.Test-Loss : 0.479,accuracy : 0.796 \n",
      "160/Train-loss : 0.147,accuracy : 0.992.Test-Loss : 0.485,accuracy : 0.794 \n",
      "170/Train-loss : 0.145,accuracy : 0.993.Test-Loss : 0.491,accuracy : 0.79 \n",
      "180/Train-loss : 0.143,accuracy : 0.996.Test-Loss : 0.496,accuracy : 0.788 \n",
      "190/Train-loss : 0.141,accuracy : 0.996.Test-Loss : 0.501,accuracy : 0.785 \n",
      "200/Train-loss : 0.139,accuracy : 0.996.Test-Loss : 0.506,accuracy : 0.783 \n",
      "210/Train-loss : 0.137,accuracy : 0.996.Test-Loss : 0.51,accuracy : 0.78 \n",
      "220/Train-loss : 0.136,accuracy : 0.996.Test-Loss : 0.514,accuracy : 0.777 \n",
      "230/Train-loss : 0.134,accuracy : 0.996.Test-Loss : 0.519,accuracy : 0.775 \n",
      "240/Train-loss : 0.133,accuracy : 0.996.Test-Loss : 0.522,accuracy : 0.773 \n",
      "250/Train-loss : 0.131,accuracy : 0.997.Test-Loss : 0.526,accuracy : 0.771 \n",
      "260/Train-loss : 0.13,accuracy : 0.997.Test-Loss : 0.53,accuracy : 0.768 \n",
      "270/Train-loss : 0.129,accuracy : 0.998.Test-Loss : 0.533,accuracy : 0.767 \n",
      "280/Train-loss : 0.127,accuracy : 0.997.Test-Loss : 0.536,accuracy : 0.766 \n",
      "290/Train-loss : 0.126,accuracy : 0.997.Test-Loss : 0.54,accuracy : 0.763 \n",
      "300/Train-loss : 0.125,accuracy : 0.997.Test-Loss : 0.543,accuracy : 0.762 \n",
      "310/Train-loss : 0.124,accuracy : 0.997.Test-Loss : 0.546,accuracy : 0.76 \n",
      "320/Train-loss : 0.123,accuracy : 0.997.Test-Loss : 0.549,accuracy : 0.759 \n",
      "330/Train-loss : 0.121,accuracy : 0.998.Test-Loss : 0.552,accuracy : 0.757 \n",
      "340/Train-loss : 0.12,accuracy : 0.998.Test-Loss : 0.554,accuracy : 0.756 \n",
      "350/Train-loss : 0.119,accuracy : 0.998.Test-Loss : 0.557,accuracy : 0.754 \n",
      "360/Train-loss : 0.118,accuracy : 0.997.Test-Loss : 0.56,accuracy : 0.752 \n",
      "370/Train-loss : 0.117,accuracy : 0.997.Test-Loss : 0.562,accuracy : 0.751 \n",
      "380/Train-loss : 0.116,accuracy : 0.997.Test-Loss : 0.564,accuracy : 0.749 \n",
      "390/Train-loss : 0.115,accuracy : 0.997.Test-Loss : 0.567,accuracy : 0.748 \n"
     ]
    }
   ],
   "source": [
    "hidden_layer =100\n",
    "num_class = 10\n",
    "learning_rate = 0.003\n",
    "size_img = 28*28\n",
    "W_1 = 0.2*np.random.random((size_img,hidden_layer)) -0.1\n",
    "W_2 = 0.2*np.random.random((hidden_layer,num_class)) - 0.1\n",
    "for epoch in range(400):\n",
    "    loss_train = 0.0\n",
    "    acc_train = 0\n",
    "    for i in range(len(images_train)):\n",
    "        L_0 = images_train[i:i+1]      \n",
    "        L_1 = relu(np.dot(L_0,W_1)) \n",
    "        L_2 =  np.dot(L_1,W_2) \n",
    "        L_2_error = (labels_train[i:i+1] - L_2)\n",
    "        L_1_error = L_2_error.dot(W_2.T)*relu_deri(L_1)\n",
    "        W_2 +=  learning_rate*L_1.T.dot(L_2_error)\n",
    "        W_1 += learning_rate*L_0.T.dot(L_1_error)\n",
    "        loss_train += np.sum((labels_train[i:i+1] -L_2)**2)\n",
    "        acc_train  += int(np.argmax(labels_train[i:i+1]) == np.argmax(L_2))\n",
    "    #print(\"Train {} : loss : {},accuracy : {}\".format(interator,round(loss/len(images_train),3),round(acc/len(images_train),3)))\n",
    "    loss_test = 0.0\n",
    "    acc_test = 0\n",
    "    for i in range(len(images_test)):\n",
    "        L_0 = images_test[i:i+1]\n",
    "        L_1 = relu(np.dot(L_0,W_1))\n",
    "        L_2 = np.dot(L_1,W_2)\n",
    "        loss_test += np.sum((labels_test[i:i+1] -L_2)**2)\n",
    "        acc_test  += int(np.argmax(labels_test[i:i+1]) == np.argmax(L_2))\n",
    "    if (epoch %10 ==0):    \n",
    "        print(\"{}/Train-loss : {},accuracy : {}.Test-Loss : {},accuracy : {} \"\n",
    "           .format(epoch,round(loss_train/len(images_train),3),round(acc_train/len(images_train),3),\n",
    "                   round(loss_test/len(images_test),3),round(acc_test/len(images_test),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dropout</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/Train-loss : 1.009,accuracy : 0.291.Test-Loss : 0.715,accuracy : 0.599 \n",
      "10/Train-loss : 0.585,accuracy : 0.667.Test-Loss : 0.498,accuracy : 0.788 \n",
      "20/Train-loss : 0.522,accuracy : 0.717.Test-Loss : 0.466,accuracy : 0.791 \n",
      "30/Train-loss : 0.509,accuracy : 0.729.Test-Loss : 0.453,accuracy : 0.804 \n",
      "40/Train-loss : 0.482,accuracy : 0.735.Test-Loss : 0.435,accuracy : 0.82 \n",
      "50/Train-loss : 0.488,accuracy : 0.772.Test-Loss : 0.454,accuracy : 0.815 \n",
      "60/Train-loss : 0.479,accuracy : 0.789.Test-Loss : 0.427,accuracy : 0.814 \n",
      "70/Train-loss : 0.482,accuracy : 0.762.Test-Loss : 0.424,accuracy : 0.825 \n",
      "80/Train-loss : 0.458,accuracy : 0.803.Test-Loss : 0.421,accuracy : 0.826 \n",
      "90/Train-loss : 0.478,accuracy : 0.776.Test-Loss : 0.419,accuracy : 0.818 \n",
      "100/Train-loss : 0.45,accuracy : 0.793.Test-Loss : 0.407,accuracy : 0.834 \n",
      "110/Train-loss : 0.46,accuracy : 0.8.Test-Loss : 0.412,accuracy : 0.824 \n",
      "120/Train-loss : 0.455,accuracy : 0.786.Test-Loss : 0.417,accuracy : 0.833 \n",
      "130/Train-loss : 0.46,accuracy : 0.792.Test-Loss : 0.43,accuracy : 0.814 \n",
      "140/Train-loss : 0.449,accuracy : 0.807.Test-Loss : 0.411,accuracy : 0.811 \n",
      "150/Train-loss : 0.451,accuracy : 0.804.Test-Loss : 0.429,accuracy : 0.824 \n",
      "160/Train-loss : 0.45,accuracy : 0.811.Test-Loss : 0.426,accuracy : 0.807 \n",
      "170/Train-loss : 0.442,accuracy : 0.803.Test-Loss : 0.424,accuracy : 0.822 \n",
      "180/Train-loss : 0.428,accuracy : 0.817.Test-Loss : 0.43,accuracy : 0.822 \n",
      "190/Train-loss : 0.43,accuracy : 0.808.Test-Loss : 0.422,accuracy : 0.817 \n",
      "200/Train-loss : 0.436,accuracy : 0.805.Test-Loss : 0.416,accuracy : 0.818 \n",
      "210/Train-loss : 0.414,accuracy : 0.834.Test-Loss : 0.422,accuracy : 0.805 \n",
      "220/Train-loss : 0.436,accuracy : 0.812.Test-Loss : 0.414,accuracy : 0.818 \n",
      "230/Train-loss : 0.425,accuracy : 0.845.Test-Loss : 0.425,accuracy : 0.818 \n",
      "240/Train-loss : 0.433,accuracy : 0.834.Test-Loss : 0.419,accuracy : 0.812 \n",
      "250/Train-loss : 0.427,accuracy : 0.83.Test-Loss : 0.416,accuracy : 0.811 \n",
      "260/Train-loss : 0.444,accuracy : 0.838.Test-Loss : 0.424,accuracy : 0.813 \n",
      "270/Train-loss : 0.405,accuracy : 0.841.Test-Loss : 0.42,accuracy : 0.812 \n",
      "280/Train-loss : 0.416,accuracy : 0.835.Test-Loss : 0.42,accuracy : 0.811 \n",
      "290/Train-loss : 0.423,accuracy : 0.83.Test-Loss : 0.429,accuracy : 0.811 \n",
      "300/Train-loss : 0.412,accuracy : 0.848.Test-Loss : 0.433,accuracy : 0.815 \n",
      "310/Train-loss : 0.4,accuracy : 0.854.Test-Loss : 0.43,accuracy : 0.806 \n",
      "320/Train-loss : 0.418,accuracy : 0.845.Test-Loss : 0.426,accuracy : 0.8 \n",
      "330/Train-loss : 0.41,accuracy : 0.838.Test-Loss : 0.434,accuracy : 0.807 \n",
      "340/Train-loss : 0.422,accuracy : 0.832.Test-Loss : 0.423,accuracy : 0.804 \n",
      "350/Train-loss : 0.418,accuracy : 0.847.Test-Loss : 0.424,accuracy : 0.806 \n",
      "360/Train-loss : 0.413,accuracy : 0.841.Test-Loss : 0.434,accuracy : 0.807 \n",
      "370/Train-loss : 0.408,accuracy : 0.843.Test-Loss : 0.416,accuracy : 0.802 \n",
      "380/Train-loss : 0.397,accuracy : 0.856.Test-Loss : 0.42,accuracy : 0.802 \n",
      "390/Train-loss : 0.409,accuracy : 0.835.Test-Loss : 0.431,accuracy : 0.796 \n"
     ]
    }
   ],
   "source": [
    "hidden_layer =100\n",
    "num_class = 10\n",
    "learning_rate = 0.003\n",
    "size_img = 28*28\n",
    "keep_prob = 0.3\n",
    "W_1 = 0.2*np.random.random((size_img,hidden_layer)) -0.1\n",
    "W_2 = 0.2*np.random.random((hidden_layer,num_class)) - 0.1\n",
    "for it in range(400):\n",
    "    loss_train = 0.0\n",
    "    acc_train = 0\n",
    "    for i in range(len(images_train)):\n",
    "        L_0 = images_train[i:i+1]      \n",
    "        L_1 = relu(np.dot(L_0,W_1)) \n",
    "        dropout = np.random.rand(L_1.shape[0],L_1.shape[1])\n",
    "        dropout = dropout < keep_prob\n",
    "        L_1 = np.multiply(L_1,dropout)\n",
    "        L_1 = L_1/keep_prob\n",
    "        L_2 =  np.dot(L_1,W_2) \n",
    "        L_2_error = (labels_train[i:i+1] - L_2)\n",
    "        L_1_error = L_2_error.dot(W_2.T)*relu_deri(L_1)\n",
    "        L_1_error = np.multiply(dropout,L_1_error)\n",
    "        L_1_error = L_1_error/keep_prob\n",
    "        W_2 +=  learning_rate*L_1.T.dot(L_2_error)\n",
    "        W_1 += learning_rate*L_0.T.dot(L_1_error)\n",
    "        loss_train += np.sum((labels_train[i:i+1] -L_2)**2)\n",
    "        acc_train  += int(np.argmax(labels_train[i:i+1]) == np.argmax(L_2))\n",
    "    loss_test = 0.0\n",
    "    acc_test = 0\n",
    "    for i in range(len(images_test)):\n",
    "        L_0 = images_test[i:i+1]\n",
    "        L_1 = relu(np.dot(L_0,W_1))\n",
    "        L_2 = np.dot(L_1,W_2)\n",
    "        loss_test += np.sum((labels_test[i:i+1] -L_2)**2)\n",
    "        acc_test  += int(np.argmax(labels_test[i:i+1]) == np.argmax(L_2))\n",
    "    if (it %10 ==0):    \n",
    "        print(\"{}/Train-loss : {},accuracy : {}.Test-Loss : {},accuracy : {} \"\n",
    "           .format(it,round(loss_train/len(images_train),3),round(acc_train/len(images_train),3),\n",
    "                   round(loss_test/len(images_test),3),round(acc_test/len(images_test),3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
